---
layout: post
title: "Heart Disease in the U.S. - Part 2"
subtitle: "Predicting heart disease in U.S. adults using Machine Learning"
background: '/img/posts/heart_p2/29058605.jpg'
---



# Recap

In the previous post, I got data from a CDC report and, after some exploratory data analysis, I created a Logistic Regression model that had an accuracy of 86% and a F1-Score of 39% as well as a ROC-AUC of 83% and AUPRC of 34%. That model is the starting point of this project. 




    The accuracy of the model is:  0.8699903063404871
    The average precision of the model is:  0.33751640563896257
    The f1-score of the model is:  0.3886683330882713
    
    
                  precision    recall  f1-score   support
    
               0       0.95      0.91      0.93     87649
               1       0.33      0.48      0.39      8290
    
        accuracy                           0.87     95939
       macro avg       0.64      0.69      0.66     95939
    weighted avg       0.89      0.87      0.88     95939
    



    
![png](/img/posts/heart_p2/output_4_1.png)
    



    
![png](/img/posts/heart_p2/output_5_0.png)
    



    
![png](/img/posts/heart_p2/output_6_0.png)
    


# Improving the previous model with different methods

The biggest problem I faced that I could potentially resolve was the imbalanced classes. There are a lot more 'negative' heart disease cases than there are 'positive' ones. This meant that even if the model guessed 'negative' for all of the cases, it would have an accuracy around 91%. The goal is to maintain accuracy around 91% but also increase the F1-Score of the program. In the first part of this project, I changed the 'class_weight' parameters in the Logistic Regression class. After doing a GridSearch of the best results of F1-Score, I managed to improve the F1-Score by 21% and got a score of 38% with an accuracy of 87%. This is a solid improvement but there is a lot of room to grow, especially by using different methods and different models. 

### Undersampling

One method that can help imbalanced classes is by undersampling the data. Undersampling the data means that it keeps all the data in the minority class ('positive' in this case) and decreases the size of the majority class to match the size of the minority. This helps balance out the classes and improves the speed of the calculations since there are less data points to go through. However this means sacrificing potentially useful data and accuracy due to less data points. 



    The accuracy of the model is:  0.7630905991232343
    The average precision of the model is:  0.8200458266437084
    The f1 score for the testing data: 0.7659830396343297
    
    
                  precision    recall  f1-score   support
    
               0       0.77      0.75      0.76      8195
               1       0.76      0.77      0.77      8229
    
        accuracy                           0.76     16424
       macro avg       0.76      0.76      0.76     16424
    weighted avg       0.76      0.76      0.76     16424
    



    
![png](/img/posts/heart_p2/output_11_1.png)
    




    
![png](/img/posts/heart_p2/output_12_0.png)
    


    
![png](/img/posts/heart_p2/output_13_0.png)
    


Already, we can see how the undersampling of the data improved the F1-Score drastically. Instead of the previous score of 38%, we now have a score of 77%. The ROC-AUC didn't improve by much, but the AUPRC did increase by a large amount from 34% to 82%. Nearly a 50% increase. 

However, as previously mentioned, the accuracy did suffer. The accuracy dropped from 87% to 76% which is not ideal. 



### Oversampling

The next method to try is oversampling. Oversamplng is the reverse of undersampling where it keeps all the data of the majority class and resamples data from the minority class. This also balances the classes out and creates a lot more data points that can be used to make accurate predictions. The cons to oversampling is that the creation of data points can create overfitting a real issue. Another problem is that there are now around 420,000 data points the program needs to run through, meaning speed is going to be much slower. Much, much slower. 



    The accuracy of the model is:  0.7632313882841086
    The average precision of the model is:  0.8199861273156904
    The f1 score for the testing data: 0.7668668275436332
    
    
                  precision    recall  f1-score   support
    
               0       0.77      0.75      0.76     87576
               1       0.76      0.78      0.77     87878
    
        accuracy                           0.76    175454
       macro avg       0.76      0.76      0.76    175454
    weighted avg       0.76      0.76      0.76    175454
    



    
![png](/img/posts/heart_p2/output_17_1.png)
    



    
![png](/img/posts/heart_p2/output_18_0.png)
    




    
![png](/img/posts/heart_p2/output_19_0.png)
    


Oversampling the data didn't improve our model from the undersampled data, it's seems to have stayed the same so we need to try different ways of improving the model. Possibly by choosing different models. 

# Using different models to improve results

The ultimate goal of this project is to have a high accuracy and F1-Score, minimizing Type II errors. We also want a very steep curve for the ROC and the AUPRC to be nearer to 100%. So far we have only used a logistic regression classifier, but there are many other classifiers that would work great for this project. 

### SVM
The first classifier we can use are Support vector machines. Support vector machines are supervised learning models with associated learning algorithms that analyze data for classification. 



    The accuracy of the model is:  0.7643199927046406
    The average precision of the model is:  0.8199378397240822
    The f1 score for the testing data: 0.7709504633501908
    
    
                  precision    recall  f1-score   support
    
               0       0.78      0.74      0.76     87576
               1       0.75      0.79      0.77     87878
    
        accuracy                           0.76    175454
       macro avg       0.77      0.76      0.76    175454
    weighted avg       0.77      0.76      0.76    175454
    



    
![png](/img/posts/heart_p2/output_24_1.png)
    




    
![png](/img/posts/heart_p2/output_25_0.png)
    




    
![png](/img/posts/heart_p2/output_26_0.png)
    


As we can see the model does not create result in any improvements. The F1-Score improved by 1% which is a small improvement, but not what we want. 

### Decision Trees

The next classifier we can try is the Decision Tree classifier. 


    The accuracy of the model is:  0.9508475155881314
    The average precision of the model is:  0.9148772430703858
    The f1 score for the testing data: 0.9531716640783657
    
    
                  precision    recall  f1-score   support
    
               0       1.00      0.90      0.95     87576
               1       0.91      1.00      0.95     87878
    
        accuracy                           0.95    175454
       macro avg       0.96      0.95      0.95    175454
    weighted avg       0.96      0.95      0.95    175454
    



    
![png](/img/posts/heart_p2/output_29_1.png)
    



    
![png](/img/posts/heart_p2/output_30_0.png)
    




    
![png](/img/posts/heart_p2/output_31_0.png)
    


After creating a Grid Search to determine the best critierion and the max-leaf depth (criterion: 'gini', max_depth=43) we can see significant improvements in the model. This is much better and more like what we want to see. 

The accuracy of this model is 95% and the F1-Score is also at 95%. The ROC-AUC is 95% and the AUPRC is also at 95%. My favorite part of this model is that out of the 200,000 data points it had to guess, there were only 109 Type II errors. That is an amazingly small amount of Type II errors. 

### Random Forest 

Although the Decision Tree classifier worked wonders, I believe that the Random Forest Classifier could do even better. 



    The accuracy of the model is:  0.9639335666328497
    The average precision of the model is:  0.9969551962911302
    The f1 score for the testing data: 0.9652128022165294
    
    
                  precision    recall  f1-score   support
    
               0       1.00      0.93      0.96     87576
               1       0.93      1.00      0.97     87878
    
        accuracy                           0.96    175454
       macro avg       0.97      0.96      0.96    175454
    weighted avg       0.97      0.96      0.96    175454
    



    
![png](/img/posts/heart_p2/output_34_1.png)
    




    
![png](/img/posts/heart_p2/output_35_0.png)



    
![png](/img/posts/heart_p2/output_36_0.png)
    


This is exactly what we were looking for. The accuracy and the F1-Score only improved by 1% to 96%, but the precision improved by 8% to 99% total meaning the model guessed nearly every 'positive' case correctly. As a result, the ROC-AUC is now at 99% and the AUPRC is at 99% as well. There is nearly no room for improvement here and it's almost better to put time into something else than improving this model since it is very good for this situation. 

Also, there are only 89 Type II errors (0.05% of Type II error) which is even better than the previous model and perfect for medical and healthcare classifiers. 

# Conclusion

At the end of this project, I have made significant improvements from where I originally started. In order to combat the class imbalance, we tried the different methods of undersampling and oversampling. Then to improve the results even further, we decided to try different models and ended up going with the Random Forest Classifier. This ended giving us an accuracy of 96%, F1-Score of 96%, precision of 99%, a ROC-AUC of 99% and an AUPRC of 99%. We had only 89 Type II errors out of nearly 200,000 data points. I am very proud of the results of this project and how well the model was able to predict heart disease.  


    The accuracy of the starting model is:  86.99903063404871
    The accuracy of the final model is:  96.43211326045574
    The accuracy improvement is:  9.43308262640702
    
    
    The average precision of the starting model is:  33.75164056389626
    The average precision of the final model is:  99.66148351791449
    The average precision improvement is:  65.90984295401825
    
    
    The f1-score of the starting model is:  38.86683330882713
    The f1-score of the final model is:  96.55771599507302
    The f1-score improement is:  57.69088268624589
    
    



    
![png](/img/posts/heart_p2/output_39_1.png)
    



```python

```
