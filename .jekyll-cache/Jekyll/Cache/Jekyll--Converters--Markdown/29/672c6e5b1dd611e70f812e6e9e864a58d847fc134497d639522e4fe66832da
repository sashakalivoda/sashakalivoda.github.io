I"˜-<h1 id="recap">Recap</h1>

<p>In the previous post, I got data from a CDC report and, after some exploratory data analysis, I created a Logistic Regression model that had an accuracy of 86% and a F1-Score of 39% as well as a ROC-AUC of 83% and AUPRC of 34%. That model is the starting point of this project.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The accuracy of the model is:  0.8699903063404871
The average precision of the model is:  0.33751640563896257
The f1-score of the model is:  0.3886683330882713


              precision    recall  f1-score   support

           0       0.95      0.91      0.93     87649
           1       0.33      0.48      0.39      8290

    accuracy                           0.87     95939
   macro avg       0.64      0.69      0.66     95939
weighted avg       0.89      0.87      0.88     95939
</code></pre></div></div>

<p><img src="/img/posts/heart_p2/output_4_1.png" alt="png" /></p>

<p><img src="/img/posts/heart_p2/output_5_0.png" alt="png" /></p>

<p><img src="/img/posts/heart_p2/output_6_0.png" alt="png" /></p>

<h1 id="improving-the-previous-model-with-different-methods">Improving the previous model with different methods</h1>

<p>The biggest problem I faced that I could potentially resolve was the imbalanced classes. There are a lot more â€˜negativeâ€™ heart disease cases than there are â€˜positiveâ€™ ones. This meant that even if the model guessed â€˜negativeâ€™ for all of the cases, it would have an accuracy around 91%. The goal is to maintain accuracy around 91% but also increase the F1-Score of the program. In the first part of this project, I changed the â€˜class_weightâ€™ parameters in the Logistic Regression class. After doing a GridSearch of the best results of F1-Score, I managed to improve the F1-Score by 21% and got a score of 38% with an accuracy of 87%. This is a solid improvement but there is a lot of room to grow, especially by using different methods and different models.</p>

<h3 id="undersampling">Undersampling</h3>

<p>One method that can help imbalanced classes is by undersampling the data. Undersampling the data means that it keeps all the data in the minority class (â€˜positiveâ€™ in this case) and decreases the size of the majority class to match the size of the minority. This helps balance out the classes and improves the speed of the calculations since there are less data points to go through. However this means sacrificing potentially useful data and accuracy due to less data points.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The accuracy of the model is:  0.7630905991232343
The average precision of the model is:  0.8200458266437084
The f1 score for the testing data: 0.7659830396343297


              precision    recall  f1-score   support

           0       0.77      0.75      0.76      8195
           1       0.76      0.77      0.77      8229

    accuracy                           0.76     16424
   macro avg       0.76      0.76      0.76     16424
weighted avg       0.76      0.76      0.76     16424
</code></pre></div></div>

<p><img src="/img/posts/heart_p2/output_11_1.png" alt="png" /></p>

<p><img src="/img/posts/heart_p2/output_12_0.png" alt="png" /></p>

<p><img src="/img/posts/heart_p2/output_13_0.png" alt="png" /></p>

<p>Already, we can see how the undersampling of the data improved the F1-Score drastically. Instead of the previous score of 38%, we now have a score of 77%. The ROC-AUC didnâ€™t improve by much, but the AUPRC did increase by a large amount from 34% to 82%. Nearly a 50% increase.</p>

<p>However, as previously mentioned, the accuracy did suffer. The accuracy dropped from 87% to 76% which is not ideal.</p>

<h3 id="oversampling">Oversampling</h3>

<p>The next method to try is oversampling. Oversamplng is the reverse of undersampling where it keeps all the data of the majority class and resamples data from the minority class. This also balances the classes out and creates a lot more data points that can be used to make accurate predictions. The cons to oversampling is that the creation of data points can create overfitting a real issue. Another problem is that there are now around 420,000 data points the program needs to run through, meaning speed is going to be much slower. Much, much slower.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The accuracy of the model is:  0.7632313882841086
The average precision of the model is:  0.8199861273156904
The f1 score for the testing data: 0.7668668275436332


              precision    recall  f1-score   support

           0       0.77      0.75      0.76     87576
           1       0.76      0.78      0.77     87878

    accuracy                           0.76    175454
   macro avg       0.76      0.76      0.76    175454
weighted avg       0.76      0.76      0.76    175454
</code></pre></div></div>

<p><img src="/img/posts/heart_p2/output_17_1.png" alt="png" /></p>

<p><img src="/img/posts/heart_p2/output_18_0.png" alt="png" /></p>

<p><img src="/img/posts/heart_p2/output_19_0.png" alt="png" /></p>

<p>Oversampling the data didnâ€™t improve our model from the undersampled data, itâ€™s seems to have stayed the same so we need to try different ways of improving the model. Possibly by choosing different models.</p>

<h1 id="using-different-models-to-improve-results">Using different models to improve results</h1>

<p>The ultimate goal of this project is to have a high accuracy and F1-Score, minimizing Type II errors. We also want a very steep curve for the ROC and the AUPRC to be nearer to 100%. So far we have only used a logistic regression classifier, but there are many other classifiers that would work great for this project.</p>

<h3 id="svm">SVM</h3>
<p>The first classifier we can use are Support vector machines. Support vector machines are supervised learning models with associated learning algorithms that analyze data for classification.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The accuracy of the model is:  0.7643199927046406
The average precision of the model is:  0.8199378397240822
The f1 score for the testing data: 0.7709504633501908


              precision    recall  f1-score   support

           0       0.78      0.74      0.76     87576
           1       0.75      0.79      0.77     87878

    accuracy                           0.76    175454
   macro avg       0.77      0.76      0.76    175454
weighted avg       0.77      0.76      0.76    175454
</code></pre></div></div>

<p><img src="/img/posts/heart_p2/output_24_1.png" alt="png" /></p>

<p><img src="/img/posts/heart_p2/output_25_0.png" alt="png" /></p>

<p><img src="/img/posts/heart_p2/output_26_0.png" alt="png" /></p>

<p>As we can see the model does not create result in any improvements. The F1-Score improved by 1% which is a small improvement, but not what we want.</p>

<h3 id="decision-trees">Decision Trees</h3>

<p>The next classifier we can try is the Decision Tree classifier.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The accuracy of the model is:  0.9508475155881314
The average precision of the model is:  0.9148772430703858
The f1 score for the testing data: 0.9531716640783657


              precision    recall  f1-score   support

           0       1.00      0.90      0.95     87576
           1       0.91      1.00      0.95     87878

    accuracy                           0.95    175454
   macro avg       0.96      0.95      0.95    175454
weighted avg       0.96      0.95      0.95    175454
</code></pre></div></div>

<p><img src="/img/posts/heart_p2/output_29_1.png" alt="png" /></p>

<p><img src="/img/posts/heart_p2/output_30_0.png" alt="png" /></p>

<p><img src="/img/posts/heart_p2/output_31_0.png" alt="png" /></p>

<p>After creating a Grid Search to determine the best critierion and the max-leaf depth (criterion: â€˜giniâ€™, max_depth=43) we can see significant improvements in the model. This is much better and more like what we want to see.</p>

<p>The accuracy of this model is 95% and the F1-Score is also at 95%. The ROC-AUC is 95% and the AUPRC is also at 95%. My favorite part of this model is that out of the 200,000 data points it had to guess, there were only 109 Type II errors. That is an amazingly small amount of Type II errors.</p>

<h3 id="random-forest">Random Forest</h3>

<p>Although the Decision Tree classifier worked wonders, I believe that the Random Forest Classifier could do even better.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The accuracy of the model is:  0.9639335666328497
The average precision of the model is:  0.9969551962911302
The f1 score for the testing data: 0.9652128022165294


              precision    recall  f1-score   support

           0       1.00      0.93      0.96     87576
           1       0.93      1.00      0.97     87878

    accuracy                           0.96    175454
   macro avg       0.97      0.96      0.96    175454
weighted avg       0.97      0.96      0.96    175454
</code></pre></div></div>

<p><img src="/img/posts/heart_p2/output_34_1.png" alt="png" /></p>

<p><img src="/img/posts/heart_p2/output_35_0.png" alt="png" /></p>

<p><img src="/img/posts/heart_p2/output_36_0.png" alt="png" /></p>

<p>This is exactly what we were looking for. The accuracy and the F1-Score only improved by 1% to 96%, but the precision improved by 8% to 99% total meaning the model guessed nearly every â€˜positiveâ€™ case correctly. As a result, the ROC-AUC is now at 99% and the AUPRC is at 99% as well. There is nearly no room for improvement here and itâ€™s almost better to put time into something else than improving this model since it is very good for this situation.</p>

<p>Also, there are only 89 Type II errors (0.05% of Type II error) which is even better than the previous model and perfect for medical and healthcare classifiers.</p>

<h1 id="conclusion">Conclusion</h1>

<p>At the end of this project, I have made significant improvements from where I originally started. In order to combat the class imbalance, we tried the different methods of undersampling and oversampling. Then to improve the results even further, we decided to try different models and ended up going with the Random Forest Classifier. This ended giving us an accuracy of 96%, F1-Score of 96%, precision of 99%, a ROC-AUC of 99% and an AUPRC of 99%. We had only 89 Type II errors out of nearly 200,000 data points. I am very proud of the results of this project and how well the model was able to predict heart disease.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The accuracy of the starting model is:  86.99903063404871
The accuracy of the final model is:  96.43211326045574
The accuracy improvement is:  9.43308262640702


The average precision of the starting model is:  33.75164056389626
The average precision of the final model is:  99.66148351791449
The average precision improvement is:  65.90984295401825


The f1-score of the starting model is:  38.86683330882713
The f1-score of the final model is:  96.55771599507302
The f1-score improement is:  57.69088268624589
</code></pre></div></div>

<p><img src="/img/posts/heart_p2/output_39_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>
:ET